# Phase 3: AI Integration Foundation - User Stories

## Epic 3.1: OpenRouter Setup

### User Story 3.1.1: API Key Management
**As a** developer
**I want to** securely store and manage my OpenRouter API key
**So that** I can use AI features without hardcoding credentials

**Acceptance Criteria:**
- [ ] API key can be entered in Settings screen
- [ ] Key is encrypted using EncryptedSharedPreferences
- [ ] Key can be cleared/updated
- [ ] Connection test validates the key
- [ ] Error message shown if key is invalid

**Technical Notes:**
- Use MasterKey with AES256_GCM
- Store in separate encrypted preferences file
- Validate key format before saving

---

### User Story 3.1.2: OpenRouter API Integration
**As a** developer
**I want to** integrate OpenRouter API client
**So that** the app can communicate with AI models

**Acceptance Criteria:**
- [ ] Retrofit interface created for OpenRouter
- [ ] Request/response models defined
- [ ] Network module configured
- [ ] API calls include proper headers (Authorization, Referer, X-Title)
- [ ] Error handling for network failures

**Technical Notes:**
- Base URL: `https://openrouter.ai/api/v1/`
- Use existing OkHttp client
- Add Moshi converter for JSON

---

## Epic 3.2: AI Service Architecture

### User Story 3.2.1: Article Summarization Service
**As a** user
**I want to** get AI-generated summaries of news articles
**So that** I can quickly understand the main points without reading the full article

**Acceptance Criteria:**
- [ ] "Summarize" button appears on article detail screen
- [ ] Clicking button generates a summary
- [ ] Loading state shown during generation
- [ ] Summary displayed below article title
- [ ] Error message shown if generation fails
- [ ] Summary is cached to avoid regeneration

**Technical Notes:**
- Use Llama 3.3 70B as default model
- Max 150 characters for summary
- Temperature: 0.3 for consistency
- Cache summaries in Room database

---

### User Story 3.2.2: AI Usage Tracking
**As a** developer
**I want to** track AI API usage
**So that** I can monitor token consumption and stay within free tier limits

**Acceptance Criteria:**
- [ ] Token usage recorded for each request
- [ ] Usage statistics viewable in Settings
- [ ] Daily/weekly/monthly totals displayed
- [ ] Warning shown when approaching rate limits
- [ ] Usage history persisted in database

**Technical Notes:**
- Track: prompt tokens, completion tokens, total tokens
- Store in AiUsageEntity with timestamp
- Free tier: 50 requests/day, 20 requests/minute

---

## Epic 3.3: Model Selection & Configuration

### User Story 3.3.1: Free Model Selection
**As a** user
**I want to** choose from available free AI models
**So that** I can select the best model for my needs

**Acceptance Criteria:**
- [ ] AI Settings section in Settings screen
- [ ] List of recommended free models shown
- [ ] Model descriptions and capabilities displayed
- [ ] Default model can be changed
- [ ] Fallback model configurable
- [ ] Model selection persists across app restarts

**Technical Notes:**
- Recommended models: Llama 3.3 70B, Gemma 2 27B, Mistral Small
- Store in AiModelPreferencesDataStore
- Show model max tokens and description

---

### User Story 3.3.2: Model Fallback
**As a** user
**I want to** have automatic fallback to alternative models
**So that** AI features continue working if the primary model fails

**Acceptance Criteria:**
- [ ] Fallback model used if primary fails
- [ ] User notified which model was used
- [ ] Fallback chain configurable
- [ ] Error logged for debugging

**Technical Notes:**
- Default fallback: Gemma 2 9B
- Retry logic: primary → fallback → error
- Log all model switches

---

## Epic 3.4: Article Summarization Feature

### User Story 3.4.1: Summary Display
**As a** user
**I want to** see AI-generated summaries in the article detail view
**So that** I can quickly decide if I want to read the full article

**Acceptance Criteria:**
- [ ] Summary appears below article title
- [ ] Summary is visually distinct from article content
- [ ] "Summarize" button only shown if no summary exists
- [ ] Cached summaries load instantly
- [ ] Summary includes timestamp and model used

**Technical Notes:**
- Use Card component for summary
- Show "Generated by [Model]" label
- Cache summaries in ArticleSummaryEntity

---

### User Story 3.4.2: Summary Caching
**As a** developer
**I want to** cache AI-generated summaries
**So that** we don't waste API calls regenerating the same summaries

**Acceptance Criteria:**
- [ ] Summaries stored in Room database
- [ ] Cached summaries retrieved before API call
- [ ] Cache linked to article ID
- [ ] Cache includes model used and timestamp
- [ ] Old summaries can be regenerated

**Technical Notes:**
- Foreign key to ArticleEntity
- Cascade delete when article removed
- Store tokens used for tracking

---

## Epic 3.5: Error Handling & Rate Limiting

### User Story 3.5.1: Rate Limit Handling
**As a** user
**I want to** be informed when I've reached API rate limits
**So that** I understand why AI features are temporarily unavailable

**Acceptance Criteria:**
- [ ] Rate limit errors detected (429 status)
- [ ] User-friendly error message shown
- [ ] Retry-after time displayed if available
- [ ] Daily limit counter shown in Settings
- [ ] Warning at 80% of daily limit

**Technical Notes:**
- Free tier: 50/day, 20/minute
- Parse Retry-After header
- Store request count in DataStore

---

### User Story 3.5.2: Offline Handling
**As a** user
**I want to** see cached summaries when offline
**So that** I can still benefit from previously generated summaries

**Acceptance Criteria:**
- [ ] Cached summaries shown when offline
- [ ] "Offline" indicator shown
- [ ] Generate button disabled when offline
- [ ] Graceful error for network failures

**Technical Notes:**
- Check network connectivity before API call
- Use NetworkMonitor from existing infrastructure
- Show cached data first, then try refresh

---

## Success Metrics

**For MVP (Phase 3 Complete):**
- ✅ 100% of articles can be summarized
- ✅ Summaries cached and reused
- ✅ API key management working
- ✅ Free tier limits respected
- ✅ Error handling covers all scenarios
- ✅ Model selection functional

**User Experience Goals:**
- Summary generation < 5 seconds
- 95% success rate for API calls
- Zero API key leaks
- Clear error messages for all failures
